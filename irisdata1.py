# -*- coding: utf-8 -*-
"""IrisData1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ev0gS15iSg0iFpp2Iu8PyroB1FhkvMB6
"""

# This is IRIS data:
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F #We use this for the loss function
from torch.utils.data import TensorDataset # We use to create the training dataset
from torch.utils.data import DataLoader # we use to batch and sheffle dataset
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# We will compare the normalisation with standard scaler
from sklearn.preprocessing import normalize
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris

'''
Onehot Encoding:
  We will need to change the labels to one hot endcode
  We will also need to revers the one hot encoding:
'''
# This function, one hot encodes the categorical labels
def onehot_encoding(data, num_rows):
  l_encoder = LabelEncoder()
  oh_encoder = OneHotEncoder(sparse=False)

  labels = l_encoder.fit_transform(data)
  data = labels.reshape(len(data), num_rows)
  onehots = oh_encoder.fit_transform(data)
  
  return onehots,  l_encoder

# The function that decodes the onehote encoded data
def decode_label(data, int_encoder):
  return int_encoder.inverse_transform([np.argmax(data)])[0]

# Get data:
iris = load_iris()
x = iris['data']
y = iris['target']

feature_names = iris['feature_names']
target_names = iris['target_names']

# Check that the data contains:
print(x.shape)
print(y.shape)
print(feature_names)
print(x[:4])
print(y[:5])

# The following will show that there are three targets: 
print(target_names)

# Let let us check unique values in the target:
# The following will confirm that there are three labels.
print(np.unique(y, axis=0))

''' Label definitions:
setosa = 0, versicolor = 1, virginica =2
'''

# Let us prepare data for further examination.
y = y.reshape(len(y), -1)
df = pd.DataFrame(np.hstack((x, y)))
df.columns = ['sepal_l', 'sepal_w', 'petal_l', 'petal_w', 'label']
df.head()

# Let us add the categorical labels:
cat_label = []
t = y.reshape(len(y),)
for i in t:
  if i == 0: 
    cat_label.append('setosa')
  elif i == 1: 
    cat_label.append('versicolor')
  else: 
    cat_label.append('virginica')

cat_label = np.array(cat_label)
cat_label = cat_label.reshape(len(cat_label), -1)

df['cat_label'] = cat_label
print(df.head())

# Let is check the relationship between the features and the labels:
sns.relplot(data=df, x='sepal_l', y='sepal_w', col='cat_label',
            hue="petal_l", style="petal_w",)
plt.show()

# Lwet us check the distributions:
sns.displot(data=df, x="sepal_l", col="cat_label", kde=True)
sns.displot(data=df, x="sepal_w", col="cat_label", kde=True)
sns.displot(data=df, x="petal_w", col="cat_label", kde=True)
plt.show()

# Let us check the relationships:
sns.pairplot(data=df, hue="label")
plt.show()

# Based on the above findings, we will onehot encode the labels
y = y.flatten()
labels, encounder = onehot_encoding(data=y, num_rows=1)

print(labels[:6])

# Let us normalise the features:
#x = normalize(x, axis=1, norm='l1')

# Let us check with the standardisation:
scaler = StandardScaler()
x = scaler.fit_transform(x)

# Convert features and targets to pytorch tensors:
features = torch.tensor(x.astype('float32'), dtype=torch.float32)
targets = torch.tensor(labels.astype('float32'), dtype=torch.float32)

# Create the train set:
traint_set = torch.utils.data.TensorDataset(features, targets)

print(features.shape)
print(targets.shape)

# User data loader to shuffel and create batches of 5
batch_size = 10
train_ld = torch.utils.data.DataLoader(dataset= traint_set, batch_size=batch_size, shuffle=True)

sample = next(iter(train_ld))

img, lb = sample
print(img.shape)
print(lb.shape)

# Build the network, defind optimiser and the loss function:
class NetWork(nn.Module):

  def __init__(self, input_dim, output_dim):
    super(NetWork, self).__init__()
    self.fc1 = nn.Linear(input_dim, 7)
    self.fc2 = nn.Linear(7, 4)
    self.drop = nn.Dropout(0.05)
    self.out = nn.Linear(4, 3)

  def forward(self, x):
    x = self.drop(F.relu(self.fc1(x)))
    x = self.drop(F.relu(self.fc2(x)))
    x = self.out(x)
    return x

# Define the model:
model = NetWork(features.shape[1], targets.shape[1])
# Define the optimiser:
optimiser = torch.optim.SGD(model.parameters(), lr=.1)

# Define the loss function:
loss_fn = nn.CrossEntropyLoss()

# run number of epochs.
epochs = 1000
losses = []

for epoch in range(epochs):

  for x_train, y_train in train_ld:
    
    optimiser.zero_grad()
    preds = model(x_train)
    loss = loss_fn(preds, torch.max(y_train, 1)[1])

    losses.append(loss.item())

    loss.backward()
    optimiser.step()

    if epoch%200 == 0:
      print(f'{epoch}/{epochs}- Loss: {loss.item()}')

      print('Plot the loss')
      plt.figure(num=epoch, figsize=(8,5))   
      plt.plot(range(len(losses)), losses)
      plt.title(f'Epoch: {epoch}; Loss: {losses[-1]}')
      plt.xlabel('Number of epochs')
      plt.ylabel('Losses')
      plt.tight_layout(pad=0.0)
plt.show()

# Decode target:
def decode_target(label, decoder):
  target = decode_label(label, encounder)
  print(target)
  if target == 0:
    return 'setosa';
  elif target == 1:
    return 'versicolor'
  else:
    return'virginica'

for l in lb:
  print(l.numpy())
  print(decode_target(l, encounder))
  print(20*'-')

# Let us evaluate the model:
num_correct = 0
totals = 0
with torch.no_grad():
  model.eval()
  for x_test, y_test in train_ld:
    for i, f in enumerate(x_test):
      print()
      p = model(f)
      print(p.numpy())
      index = p.argmax()
      print(index)

      if index == 0:
        label = [1., 0., 0.]
      elif index == 1:
        label = [0., 1., 0.]
      else:
        label = [0., 0., 1.]

      target = decode_target(label, encounder)
      print(target)
    
      print(f'Predict: {label}')
      print(f'Actual : {y_test[i].numpy()}')

      totals +=1
      if (label == y_test[i].numpy()).sum() == 3:
        num_correct +=1
        print('Correct')
      else:
        print('Wrong')

      print(20*'-')
      #break

print(num_correct)
print(f'Total: {totals}')
print(f'Accuracy: {num_correct/totals}')

Predict = [0.0, 0.0, 1.0]
Actual = [0., 0., 1.]

Predict == Actual

print(label)  
print(y_test[i].numpy())

(label == y_test[i].numpy()).sum()

path = 'iris_model1.pt'
torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimiser.state_dict(),
            'loss': loss
            }, path)

test_model =  NetWork(features.shape[1], targets.shape[1])
test_optimiser = nn.CrossEntropyLoss()

checkPoint = torch.load(path)

# Get the model, optimiser and the loss:
test_model.load_state_dict(checkPoint['model_state_dict'])
test_optimiser = checkPoint['optimizer_state_dict']
test_loss = checkPoint['loss']
test_epoch = checkPoint['epoch']

list(test_model.parameters())

list(model.parameters())